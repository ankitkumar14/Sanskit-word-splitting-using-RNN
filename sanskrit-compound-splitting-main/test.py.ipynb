{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sticky-taxation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26714/26714 [==============================] - 2148s 80ms/step - loss: 0.1064 - sparse_categorical_accuracy: 0.9723 - hamming_distance: 7.0856\n",
      "Test loss:  0.10725626349449158\n",
      "Test accuracy:  0.9721819162368774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from data import DataLoader\n",
    "from model import Model\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "seed_value = 12\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "VALIDATION_DATA_PATH = os.path.join(DATA_DIR, \"validation.csv\")\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_UNITS = 256\n",
    "DROPOUT_PROBABILITY = 0.2\n",
    "FILTER_SIZES = [3, 5, 7]\n",
    "NUM_FILTERS = 128\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LERANING_RATE = 0.005\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"ckpt_{epoch:03d}\")\n",
    "\n",
    "LOG_DIR = \"logs\"\n",
    "\n",
    "\n",
    "test_dataset = DataLoader(\n",
    "    TEST_DATA_PATH, max_length=MAX_LENGTH, batch_size=1\n",
    ").get_dataset()\n",
    "\n",
    "INPUT_TOKENIZER_PATH = os.path.join(\"tokenizers\", \"input_tokenizer.json\")\n",
    "TARGET_TOKENIZER_PATH = os.path.join(\"tokenizers\", \"target_tokenizer.json\")\n",
    "\n",
    "with open(INPUT_TOKENIZER_PATH) as f:\n",
    "    data = json.load(f)\n",
    "    input_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "with open(TARGET_TOKENIZER_PATH) as f:\n",
    "    data = json.load(f)\n",
    "    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "\n",
    "INPUT_VOCAB_SIZE = len(input_tokenizer.word_index) + 1\n",
    "TARGET_VOCAB_SIZE = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "model = Model(\n",
    "    BATCH_SIZE,\n",
    "    INPUT_VOCAB_SIZE,\n",
    "    EMBEDDING_DIM,\n",
    "    RNN_UNITS,\n",
    "    DROPOUT_PROBABILITY,\n",
    "    FILTER_SIZES,\n",
    "    NUM_FILTERS,\n",
    "    MAX_LENGTH,\n",
    "    TARGET_VOCAB_SIZE,\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LERANING_RATE)\n",
    "\n",
    "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "class HammingDistance(tf.keras.metrics.Mean):\n",
    "    def __init__(self, name=\"hamming_distance\"):\n",
    "        super().__init__(name=name)\n",
    "        self._fn = self.hamming_distance\n",
    "        self.__name__ = name\n",
    "\n",
    "    def hamming_distance(self, y_true, y_pred):\n",
    "        y_pred = tf.cast(\n",
    "            tf.argmax(tf.nn.softmax(y_pred, axis=2), axis=2), dtype=y_true.dtype\n",
    "        )\n",
    "        result = tf.not_equal(y_true, y_pred)\n",
    "        not_eq = tf.reduce_sum(tf.cast(result, tf.float32))\n",
    "        ham_distance = tf.math.divide_no_nan(not_eq, result.shape[0])\n",
    "        return ham_distance\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, self._dtype)\n",
    "        y_pred = tf.cast(y_pred, self._dtype)\n",
    "        matches = self._fn(y_true, y_pred)\n",
    "        return super().update_state(matches, sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function,\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(), HammingDistance()],\n",
    ")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "\n",
    "latest = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
    "model.load_weights(latest).expect_partial()\n",
    "\n",
    "test_loss, test_accuracy ,*is_anything_else_being_returned = model.evaluate(test_dataset)\n",
    "print(\"Test loss: \", test_loss)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-burke",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
